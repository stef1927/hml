{
 "metadata": {
  "name": "",
  "signature": "sha256:040974368f4fda96653cb814ac691a6a6c507172ec1a01635570ff74893b8829"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os,sys, time\n",
      "import random,string,math,csv\n",
      "import numpy as np\n",
      "import scipy as sci"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Read the traing set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all = list(csv.reader(open(\"../data/training.csv\",\"rb\"), delimiter=','))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perform feature scaling ((x - min) / (max - min)) * 1000 so that all features are between 0 and 1000, missing values become -1. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalize(inData, c1, c2):\n",
      "    ret = np.array([map(float, row[c1 : c2]) for row in inData])\n",
      "    ret[ret == -999.0] = np.nan\n",
      "\n",
      "    ret_min = np.nanmin(ret,0)\n",
      "    ret_max = np.nanmax(ret,0)\n",
      "\n",
      "    ret = ((ret - ret_min) / (ret_max - ret_min)) * 1000 \n",
      "    ret[np.isnan(ret)] = -1\n",
      "    return ret"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights = np.array([float(row[-2]) for row in all[1:]])\n",
      "labels = np.array([map(lambda l: 1.0 if l == 's' else 0.0, row[-1]) for row in all[1:]]).flatten()\n",
      "\n",
      "xs = normalize(all[1:], 1, -2)\n",
      "(numPoints,numFeatures) = xs.shape\n",
      "\n",
      "#print xs.shape\n",
      "#print xs\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Build training and validation sets:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sIndexes = labels == 1.0\n",
      "bIndexes = labels == 0.0\n",
      "\n",
      "sumWeights = np.sum(weights)\n",
      "sumSWeights = np.sum(weights[sIndexes])\n",
      "sumBWeights = np.sum(weights[bIndexes])\n",
      "\n",
      "#print numPoints, sumWeights, sumSWeights, sumBWeights\n",
      "\n",
      "randomPermutation = random.sample(range(len(xs)), len(xs))\n",
      "#np.savetxt(\"randomPermutation.csv\",randomPermutation,fmt='%d',delimiter=',')\n",
      "#randomPermutation = np.array(map(int,np.array(list(csv.reader(open(\"randomPermutation.csv\",\"rb\"), delimiter=','))).flatten()))\n",
      "\n",
      "numPointsTrain = int(numPoints*0.8)\n",
      "numPointsValidation = (numPoints - numPointsTrain) / 2\n",
      "numPointsTest = numPointsValidation\n",
      "\n",
      "print numPointsTrain, numPointsValidation, numPointsTest\n",
      "\n",
      "xsTrain = xs[randomPermutation[:numPointsTrain]]\n",
      "xsValidation = xs[randomPermutation[numPointsTrain:numPointsTrain+numPointsValidation]]\n",
      "xsTest = xs[randomPermutation[numPointsTrain+numPointsValidation:]]\n",
      "\n",
      "sSelectorTrain = sIndexes[randomPermutation[:numPointsTrain]]\n",
      "bSelectorTrain = bIndexes[randomPermutation[:numPointsTrain]]\n",
      "sSelectorValidation = sIndexes[randomPermutation[numPointsTrain:numPointsTrain+numPointsValidation]]\n",
      "bSelectorValidation = bIndexes[randomPermutation[numPointsTrain:numPointsTrain+numPointsValidation]]\n",
      "sSelectorTest = sIndexes[randomPermutation[numPointsTrain+numPointsValidation:]]\n",
      "bSelectorTest = bIndexes[randomPermutation[numPointsTrain+numPointsValidation:]]\n",
      "\n",
      "weightsTrain = weights[randomPermutation[:numPointsTrain]]\n",
      "weightsValidation = weights[randomPermutation[numPointsTrain:numPointsTrain+numPointsValidation]]\n",
      "weightsTest = weights[randomPermutation[numPointsTrain+numPointsValidation:]]\n",
      "\n",
      "labelsTrain = labels[randomPermutation[:numPointsTrain]]\n",
      "labelsValidation = labels[randomPermutation[numPointsTrain:numPointsTrain+numPointsValidation]]\n",
      "labelsTest = labels[randomPermutation[numPointsTrain+numPointsValidation:]]\n",
      "\n",
      "sumWeightsTrain = np.sum(weightsTrain)\n",
      "sumSWeightsTrain = np.sum(weightsTrain[sSelectorTrain])\n",
      "sumBWeightsTrain = np.sum(weightsTrain[bSelectorTrain])\n",
      "\n",
      "print sumWeightsTrain, sumSWeightsTrain, sumBWeightsTrain"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "200000 25000 25000\n",
        "329227.804144"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 553.900124737 328673.904019\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perform logistic regression using theano"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano\n",
      "import theano.tensor as T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogisticRegression(object):\n",
      "    \n",
      "    def __init__(self, input, n_in, n_out):\n",
      "        \"\"\" Initialize the parameters of the logistic regression\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "                      architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "                     which the datapoints lie\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "                      which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # The weights W as a matrix of shape (n_in, n_out)\n",
      "        self.W = theano.shared(value=np.zeros((n_in, n_out),\n",
      "                            dtype=theano.config.floatX),\n",
      "                            name='W', borrow=True)\n",
      "        # The baises b as a vector of n_out elements\n",
      "        self.b = theano.shared(value=np.zeros((n_out,),\n",
      "                            dtype=theano.config.floatX),\n",
      "                            name='b', borrow=True)\n",
      "\n",
      "        # compute vector of class-membership probabilities in symbolic form\n",
      "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
      "        # self.p_y_given_x = T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
      "\n",
      "        # compute prediction as class whose probability is maximal in symbolic form\n",
      "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
      "\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "    #def sigmoid_cost(self, y):    \n",
      "    #    return -T.mean(y[T.arange(y.shape[0])] * T.log(self.p_y_given_x[T.arange(y.shape[0])]) +\n",
      "    #        (1 - y[T.arange(y.shape[0])]) * T.log(1 - self.p_y_given_x[T.arange(y.shape[0])]))\n",
      "    \n",
      "    def negative_log_likelihood(self, y):\n",
      "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
      "        # number of examples (call it n) in the minibatch\n",
      "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
      "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
      "        # Log-Probabilities (call it LP) with one row per example and\n",
      "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
      "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
      "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
      "        # the mean (across minibatch examples) of the elements in v,\n",
      "        # i.e., the mean log-likelihood across the minibatch.\n",
      "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
      "\n",
      "    def errors(self, y):\n",
      "        # check if y has same dimension of y_pred\n",
      "        if y.ndim != self.y_pred.ndim:\n",
      "            raise TypeError('y should have the same shape as self.y_pred',\n",
      "                ('y', target.type, 'y_pred', self.y_pred.type))\n",
      "        # check if y is of the correct datatype\n",
      "        if y.dtype.startswith('int'):\n",
      "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
      "            # represents a mistake in prediction\n",
      "            return T.mean(T.neq(self.y_pred, y))\n",
      "        else:\n",
      "            raise NotImplementedError()\n",
      "            \n",
      "            \n",
      "    def scores(self):\n",
      "        return self.y_pred\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def shared_dataset(data_x, data_y, borrow=True):\n",
      "        \"\"\" Function that loads the dataset into shared variables\n",
      "\n",
      "        The reason we store our dataset in shared variables is to allow\n",
      "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
      "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
      "        is needed (the default behaviour if the data is not in a shared\n",
      "        variable) would lead to a large decrease in performance.\n",
      "        \"\"\"\n",
      "        shared_x = theano.shared(np.asarray(data_x, dtype=theano.config.floatX), borrow=borrow)\n",
      "        shared_y = theano.shared(np.asarray(data_y, dtype=theano.config.floatX), borrow=borrow)\n",
      "        \n",
      "        # When storing data on the GPU it has to be stored as floats\n",
      "        # therefore we will store the labels as ``floatX`` as well\n",
      "        # (``shared_y`` does exactly that). But during our computations\n",
      "        # we need them as ints (we use labels as index, and if they are\n",
      "        # floats it doesn't make sense) therefore instead of returning\n",
      "        # ``shared_y`` we will have to cast it to int. This little hack\n",
      "        # lets ous get around this issue\n",
      "        return shared_x, T.cast(shared_y, 'int32')\n",
      "\n",
      "train_set_x, train_set_y = shared_dataset(xsTrain, labelsTrain)\n",
      "valid_set_x, valid_set_y = shared_dataset(xsValidation, labelsValidation)\n",
      "test_set_x,  test_set_y = shared_dataset(xsTest, labelsTest)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None, activation=T.tanh):\n",
      "        \"\"\"\n",
      "        Typical hidden layer of a MLP: units are fully-connected and have\n",
      "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
      "        and the bias vector b is of shape (n_out,).\n",
      "\n",
      "        NOTE : The nonlinearity used here is tanh\n",
      "\n",
      "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
      "\n",
      "        :type rng: np.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.dmatrix\n",
      "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: dimensionality of input\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of hidden units\n",
      "\n",
      "        :type activation: theano.Op or function\n",
      "        :param activation: Non linearity to be applied in the hidden\n",
      "                           layer\n",
      "        \"\"\"\n",
      "        self.input = input\n",
      "\n",
      "        # `W` is initialized with `W_values` which is uniformely sampled\n",
      "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
      "        # for tanh activation function\n",
      "        # the output of uniform if converted using asarray to dtype\n",
      "        # theano.config.floatX so that the code is runable on GPU\n",
      "        # Note : optimal initialization of weights is dependent on the\n",
      "        #        activation function used (among other things).\n",
      "        #        For example, results presented in [Xavier10] suggest that you\n",
      "        #        should use 4 times larger initial weights for sigmoid\n",
      "        #        compared to tanh\n",
      "        #        We have no info for other function, so we use the same as\n",
      "        #        tanh.\n",
      "        if W is None:\n",
      "            W_values = np.asarray(rng.uniform(\n",
      "                    low=-np.sqrt(6. / (n_in + n_out)),\n",
      "                    high=np.sqrt(6. / (n_in + n_out)),\n",
      "                    size=(n_in, n_out)), dtype=theano.config.floatX)\n",
      "            if activation == theano.tensor.nnet.sigmoid:\n",
      "                W_values *= 4\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = np.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (lin_output if activation is None\n",
      "                       else activation(lin_output))\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MLP(object):\n",
      "    \"\"\"Multi-Layer Perceptron Class\n",
      "\n",
      "    A multilayer perceptron is a feedforward artificial neural network model\n",
      "    that has one layer or more of hidden units and nonlinear activations.\n",
      "    Intermediate layers usually have as activation function tanh or the\n",
      "    sigmoid function (defined here by a ``HiddenLayer`` class)  while the\n",
      "    top layer is a softamx layer (defined here by a ``LogisticRegression``\n",
      "    class).\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
      "        # into a HiddenLayer with a tanh activation function connected to the\n",
      "        # LogisticRegression layer; the activation function can be replaced by\n",
      "        # sigmoid or any other nonlinear function\n",
      "        self.hiddenLayer = HiddenLayer(rng=rng, input=input,\n",
      "                                       n_in=n_in, n_out=n_hidden,\n",
      "                                       activation=T.tanh)\n",
      "\n",
      "        # The logistic regression layer gets as input the hidden units\n",
      "        # of the hidden layer\n",
      "        self.logRegressionLayer = LogisticRegression(\n",
      "            input=self.hiddenLayer.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out)\n",
      "\n",
      "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
      "        # be small\n",
      "        self.L1 = abs(self.hiddenLayer.W).sum() \\\n",
      "                + abs(self.logRegressionLayer.W).sum()\n",
      "\n",
      "        # square of L2 norm ; one regularization option is to enforce\n",
      "        # square of L2 norm to be small\n",
      "        self.L2_sqr = (self.hiddenLayer.W ** 2).sum() \\\n",
      "                    + (self.logRegressionLayer.W ** 2).sum()\n",
      "\n",
      "        # negative log likelihood of the MLP is given by the negative\n",
      "        # log likelihood of the output of the model, computed in the\n",
      "        # logistic regression layer\n",
      "        self.negative_log_likelihood = self.logRegressionLayer.negative_log_likelihood\n",
      "        # same holds for the function computing the number of errors\n",
      "        self.errors = self.logRegressionLayer.errors\n",
      "\n",
      "        # the parameters of the model are the parameters of the two layer it is\n",
      "        # made out of\n",
      "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
      "        \n",
      "        self.scores = self.logRegressionLayer.scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000, batch_size=20, n_hidden=500):\n",
      "    \"\"\"\n",
      "    Demonstrate stochastic gradient descent optimization for a multilayer\n",
      "    perceptron\n",
      "\n",
      "    :type learning_rate: float\n",
      "    :param learning_rate: learning rate used (factor for the stochastic\n",
      "    gradient\n",
      "\n",
      "    :type L1_reg: float\n",
      "    :param L1_reg: L1-norm's weight when added to the cost (see\n",
      "    regularization)\n",
      "\n",
      "    :type L2_reg: float\n",
      "    :param L2_reg: L2-norm's weight when added to the cost (see\n",
      "    regularization)\n",
      "\n",
      "    :type n_epochs: int\n",
      "    :param n_epochs: maximal number of epochs to run the optimizer\n",
      "   \"\"\"\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "\n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print('Building the model, num train batches %i, num valid batches %i' % \\\n",
      "        (n_train_batches, n_valid_batches));\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.matrix('x')  # the data is presented as rasterized images\n",
      "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
      "                        # [int] labels\n",
      "\n",
      "    rng = np.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    classifier = MLP(rng=rng, input=x, n_in=numFeatures, n_hidden=n_hidden, n_out=2)\n",
      "\n",
      "    # the cost we minimize during training is the negative log likelihood of\n",
      "    # the model plus the regularization terms (L1 and L2); cost is expressed\n",
      "    # here symbolically\n",
      "    cost = classifier.negative_log_likelihood(y) \\\n",
      "         + L1_reg * classifier.L1 \\\n",
      "         + L2_reg * classifier.L2_sqr\n",
      "\n",
      "    # classification errors\n",
      "    test_error_model = theano.function(inputs=[index],\n",
      "            outputs=classifier.errors(y),\n",
      "            givens={\n",
      "                x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "                y: test_set_y[index * batch_size:(index + 1) * batch_size]})\n",
      "\n",
      "    validate_error_model = theano.function(inputs=[index],\n",
      "            outputs=classifier.errors(y),\n",
      "            givens={\n",
      "                x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "                y: valid_set_y[index * batch_size:(index + 1) * batch_size]})\n",
      "    \n",
      "    train_error_model = theano.function(inputs=[index],\n",
      "            outputs=classifier.errors(y),\n",
      "            givens={\n",
      "                x: train_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "                y: train_set_y[index * batch_size:(index + 1) * batch_size]})\n",
      "\n",
      "    # classification scores\n",
      "    test_score_model = theano.function(inputs=[index],\n",
      "            outputs=classifier.scores,\n",
      "            givens={\n",
      "                x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "                y: test_set_y[index * batch_size:(index + 1) * batch_size]})\n",
      "    \n",
      "        \n",
      "    # compute the gradient of cost with respect to theta (sotred in params)\n",
      "    # the resulting gradients will be stored in a list gparams\n",
      "    gparams = []\n",
      "    for param in classifier.params:\n",
      "        gparam = T.grad(cost, param)\n",
      "        gparams.append(gparam)\n",
      "\n",
      "    # specify how to update the parameters of the model as a list of\n",
      "    # (variable, update expression) pairs\n",
      "    updates = []\n",
      "    # given two list the zip A = [a1, a2, a3, a4] and B = [b1, b2, b3, b4] of\n",
      "    # same length, zip generates a list C of same size, where each element\n",
      "    # is a pair formed from the two lists :\n",
      "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
      "    for param, gparam in zip(classifier.params, gparams):\n",
      "        updates.append((param, param - learning_rate * gparam))\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(inputs=[index], outputs=cost,\n",
      "            updates=updates,\n",
      "            givens={\n",
      "                x: train_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "                y: train_set_y[index * batch_size:(index + 1) * batch_size]})\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training'\n",
      "\n",
      "    # early-stopping parameters\n",
      "    patience = 20000  # look as this many examples regardless\n",
      "    patience_increase = 2  # wait this much longer when a new best is found\n",
      "    improvement_threshold = 0.995  # a relative improvement of this much is considered significant\n",
      "    validation_frequency = min(n_train_batches, patience / 2)\n",
      "                                  # go through this many\n",
      "                                  # minibatche before checking the network\n",
      "                                  # on the validation set; in this case we\n",
      "                                  # check every epoch\n",
      "\n",
      "    best_params = None\n",
      "    best_test_loss = np.inf\n",
      "    best_validation_loss = np.inf\n",
      "    best_train_loss = np.inf\n",
      "    best_iter = 0\n",
      "    test_score = 0.\n",
      "    start_time = time.clock()\n",
      "\n",
      "    epoch = 0\n",
      "    done_looping = False\n",
      "    \n",
      "    errorsTrain = np.zeros((n_epochs+1))\n",
      "    errorsValidation = np.zeros((n_epochs+1))    \n",
      "\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "\n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            # iteration number\n",
      "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
      "\n",
      "            if (iter + 1) % validation_frequency == 0:\n",
      "                # compute zero-one loss on validation set\n",
      "                validation_losses = [validate_error_model(i) for i in xrange(n_valid_batches)]\n",
      "                this_validation_loss = np.mean(validation_losses)\n",
      "\n",
      "                # if we got the best validation score until now\n",
      "                if this_validation_loss < best_validation_loss:\n",
      "                    #improve patience if loss improvement is good enough\n",
      "                    if this_validation_loss < best_validation_loss * improvement_threshold:\n",
      "                        patience = max(patience, iter * patience_increase)\n",
      "\n",
      "                    best_validation_loss = this_validation_loss\n",
      "                    \n",
      "                    train_losses = [train_error_model(i) for i in xrange(n_valid_batches)]\n",
      "                    best_train_loss = np.mean(train_losses)\n",
      "                    \n",
      "                    test_scores = [test_score_model(i) for i in xrange(n_test_batches)]\n",
      "                    test_losses = [test_error_model(i) for i in xrange(n_test_batches)]\n",
      "                    \n",
      "                    best_test_loss = np.mean(test_losses)\n",
      "                    best_iter = iter\n",
      "\n",
      "                print('epoch %i, minibatch %i/%i, patience %i, iter %i, test error %f %%, valid error %f %%' %\n",
      "                     (epoch, minibatch_index + 1, n_train_batches, patience, iter, \n",
      "                      best_test_loss * 100., best_validation_loss * 100.))     \n",
      "\n",
      "            if patience <= iter:\n",
      "                done_looping = True\n",
      "                break\n",
      "                    \n",
      "        errorsTrain[epoch] = best_train_loss\n",
      "        errorsValidation[epoch] = best_validation_loss\n",
      "        \n",
      "    end_time = time.clock()\n",
      "    print(('Optimization complete. Best validation score of %f %% '\n",
      "           'obtained at iteration %i, with test performance %f %%') %\n",
      "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
      "    print >> sys.stderr, ('The code ran for %.2fm' % ((end_time - start_time) / 60.))\n",
      "    \n",
      "    return (epoch, errorsTrain, errorsValidation, test_scores);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epoch, errorsTrain, errorsValidation , test_scores = \\\n",
      "    test_mlp(learning_rate=0.001, L1_reg=0.00, L2_reg=0.0005, n_epochs=50, batch_size=50, n_hidden=100)\n",
      "    \n",
      "print test_scores    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Building the model, num train batches 4000, num valid batches 500\n"
       ]
      },
      {
       "ename": "TypeError",
       "evalue": "('output must be a theano Variable or Out instance (or list of them)', <bound method LogisticRegression.scores of <__main__.LogisticRegression object at 0x31e1b86c>>)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-52-9ba01759f448>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrorsTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrorsValidation\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m     \u001b[0mtest_mlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL1_reg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL2_reg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtest_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-51-781d201f82e2>\u001b[0m in \u001b[0;36mtest_mlp\u001b[1;34m(learning_rate, L1_reg, L2_reg, n_epochs, batch_size, n_hidden)\u001b[0m\n\u001b[0;32m     70\u001b[0m             givens={\n\u001b[0;32m     71\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_set_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                 y: test_set_y[index * batch_size:(index + 1) * batch_size]})\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    221\u001b[0m                 \u001b[0mallow_input_downcast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_input_downcast\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 profile=profile)\n\u001b[0m\u001b[0;32m    224\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    488\u001b[0m                                          \u001b[0mrebuild_strict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrebuild_strict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m                                          \u001b[0mcopy_inputs_over\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                                          no_default_updates=no_default_updates)\n\u001b[0m\u001b[0;32m    491\u001b[0m     \u001b[1;31m# extracting the arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[0minput_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcloned_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_stuff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_vars\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mrebuild_collect_shared\u001b[1;34m(outputs, inputs, replace, updates, rebuild_strict, copy_inputs_over, no_default_updates)\u001b[0m\n\u001b[0;32m    252\u001b[0m             raise TypeError('output must be a theano Variable or Out '\n\u001b[0;32m    253\u001b[0m                             \u001b[1;34m'instance (or list of them)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                             outputs)\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;31m# Iterate over update_expr, cloning its elements, and updating\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: ('output must be a theano Variable or Out instance (or list of them)', <bound method LogisticRegression.scores of <__main__.LogisticRegression object at 0x31e1b86c>>)"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plot the learning errors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "fig.suptitle('Logistic regression learning curves', fontsize=14, fontweight='bold')\n",
      "ax = fig.add_subplot(111)\n",
      "fig.subplots_adjust(top=0.85)\n",
      "\n",
      "ax.set_xlabel('number of epochs')\n",
      "ax.set_ylabel('errors')\n",
      "\n",
      "ax.plot(range(epoch), errorsTrain[:epoch] * 100, 'b-')\n",
      "ax.plot(range(epoch), errorsValidation[:epoch] * 100, 'r-')\n",
      "\n",
      "ax.axis([0, epoch, 0, 100])\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 123
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate validation scores and sort them in increasing order of the scores:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scoresValidation = net.activateOnDataset(dsValidation)[:, 1]\n",
      "#print scoresValidation.transpose()\n",
      "\n",
      "tIIs = scoresValidation.argsort()\n",
      "#print tIIs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We define the Approximate Median Significance:\n",
      "\n",
      "\\begin{equation*}\n",
      "\\text{AMS} = \\sqrt{ 2 \\left( (s + b + 10) \\ln \\left( 1 + \\frac{s}{b +\n",
      "    10} \\right) - s \\right) }\n",
      "\\end{equation*}\n",
      "\n",
      "where <code>s</code> and <code>b</code> are the sum of signal and background weights, respectively, in the selection region."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def AMS(s,b):\n",
      "    assert s >= 0\n",
      "    assert b >= 0\n",
      "    bReg = 10.\n",
      "    return math.sqrt(2 * ((s + b + bReg) * math.log(1 + s / (b + bReg)) - s))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Weights have to be normalized to the same sum as in the full set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wFactor = 1.* numPoints / numPointsValidation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Initializing $s$ and $b$ to the full sum of weights, we start by having all points in the selection region. <code>amss</code> will contain AMSs after each point moved out of the selection region in the sorted validation set. <code>amsMax</code> will contain the best validation AMS, and <code>threshold</code> will be the smallest score among the selected points. We will do <code>len(tIIs)</code> iterations, which means that <code>amss[-1]</code> is the AMS when only the point with the highest score is selected."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = np.sum(weightsValidation[sSelectorValidation])\n",
      "b = np.sum(weightsValidation[bSelectorValidation])\n",
      "amss = np.empty([len(tIIs)])\n",
      "amsMax = 0\n",
      "threshold = 0.0\n",
      "for tI in range(len(tIIs)):\n",
      "    # don't forget to renormalize the weights to the same sum \n",
      "    # as in the complete training set\n",
      "    amss[tI] = AMS(max(0,s * wFactor),max(0,b * wFactor))\n",
      "    # careful with small regions, they fluctuate a lot\n",
      "    if tI < 0.9 * len(tIIs) and amss[tI] > amsMax:\n",
      "        amsMax = amss[tI]\n",
      "        threshold = scoresValidation[tIIs[tI]]\n",
      "        #print tI,threshold\n",
      "    if sSelectorValidation[tIIs[tI]]:\n",
      "        s -= weightsValidation[tIIs[tI]]\n",
      "    else:\n",
      "        b -= weightsValidation[tIIs[tI]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plotting the AMS vs the rank."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure()\n",
      "fig.suptitle('AMS curves', fontsize=14, fontweight='bold')\n",
      "vsRank = fig.add_subplot(111)\n",
      "fig.subplots_adjust(top=0.85)\n",
      "\n",
      "vsRank.set_xlabel('rank')\n",
      "vsRank.set_ylabel('AMS')\n",
      "\n",
      "vsRank.plot(amss,'b-')\n",
      "\n",
      "vsRank.axis([0,len(amss), 0, 4])\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plotting the AMS vs the score."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure()\n",
      "fig.suptitle('AMS curves', fontsize=14, fontweight='bold')\n",
      "vsScore = fig.add_subplot(111)\n",
      "fig.subplots_adjust(top=0.85)\n",
      "\n",
      "vsScore.set_xlabel('score')\n",
      "vsScore.set_ylabel('AMS')\n",
      "\n",
      "vsScore.plot(scoresValidation[tIIs],amss,'b-')\n",
      "\n",
      "vsScore.axis([scoresValidation[tIIs[0]], scoresValidation[tIIs[-1]] , 0, 4])\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Constructing the submission file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allTest = list(csv.reader(open(\"../data/test.csv\",\"rb\"), delimiter=','))\n",
      "\n",
      "xsTest = normalize(allTest[1:], 1, 31)\n",
      "(numTestPoints, numTestFeatures) = xsTest.shape\n",
      "\n",
      "print numTestPoints, numFeatures, numTestFeatures"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "550000 30 30\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testIds = np.array([int(row[0]) for row in allTest[1:]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testScores = np.zeros(numTestPoints)\n",
      "\n",
      "for i in range(numTestPoints):\n",
      "    testScores[i] = net.activate(xsTest[i])[1]\n",
      "\n",
      "#print testScores.transpose()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Computing the rank order."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testInversePermutation = testScores.argsort()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testPermutation = list(testInversePermutation)\n",
      "for tI,tII in zip(range(len(testInversePermutation)), testInversePermutation):\n",
      "    testPermutation[tII] = tI"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Computing the submission file with columns EventId, RankOrder, and Class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "submission = np.array([[str(testIds[tI]),str(testPermutation[tI]+1),\n",
      "                       's' if testScores[tI] >= threshold else 'b'] \n",
      "            for tI in range(len(testIds))])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "submission = np.append([['EventId','RankOrder','Class']], submission, axis=0)\n",
      "\n",
      "np.savetxt(\"submission.csv\",submission,fmt='%s',delimiter=',')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}